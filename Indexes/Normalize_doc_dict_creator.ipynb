{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ],
      "metadata": {
        "id": "d9XnTkHQymNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K8WSZN6xdst"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkEErqJnsXyy"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "pAOgKKOxzJqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'ir-bucket-1' \n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name, prefix = 'wiki_files/')\n",
        "for b in blobs:\n",
        "    if b.name != 'wiki_files/graphframes.sh' and b.name != 'wiki_files/':\n",
        "        paths.append(full_path+b.name)\n"
      ],
      "metadata": {
        "id": "piqiJ0mCzyW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if\n",
        "                      token.group() not in all_stopwords]\n",
        "    return list_of_tokens"
      ],
      "metadata": {
        "id": "RdKQdMq3z3oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "\n",
        "# Count number of wiki pages\n",
        "N_docs = parquetFile.count()"
      ],
      "metadata": {
        "id": "Twxc1BCb7Q4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math \n",
        "from collections import defaultdict\n",
        "\n",
        "def doc_id_normalize(tokenize_text):\n",
        "    d = defaultdict(int)\n",
        "    for token in list(tokenize_text):\n",
        "        d[token] += 1\n",
        "    n = 0\n",
        "    for token in d.keys():\n",
        "        n += d[token]**2   \n",
        "    if n == 0:\n",
        "        return n\n",
        "    return 1/math.sqrt(n)"
      ],
      "metadata": {
        "id": "OyehiaLu2X7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_text_pairs = parquetFile.select(\"id\", \"text\").rdd\n",
        "\n",
        "n_doc_text_pairs = doc_text_pairs.mapValues(tokenize).mapValues(doc_id_normalize)\n",
        "\n",
        "normalize_doc_dict= dict(n_doc_text_pairs.collect())"
      ],
      "metadata": {
        "id": "gOPkvYHT2Grh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "name = \"normalize_doc_dict\"\n",
        "with open(f\"{name}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(normalize_doc_dict, f)\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob_posting_locs = bucket.blob(f\"normalze_dict/{name}.pkl\")\n",
        "blob_posting_locs.upload_from_filename(f\"{name}.pkl\")"
      ],
      "metadata": {
        "id": "evUlPpGo8g9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}