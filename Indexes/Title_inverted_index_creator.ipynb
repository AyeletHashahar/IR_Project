{"cells":[{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":1,"id":"bf199e6a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"6b7169d3-0fa0-4a51-e005-a14736e0a187"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: nltk in /opt/conda/miniconda3/lib/python3.8/site-packages (3.6.3)\n","Requirement already satisfied: regex in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n","Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n","Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (1.2.0)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install nltk"]},{"cell_type":"code","execution_count":2,"id":"d8f56ecd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387},"id":"d8f56ecd","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8ebe1d94-a63b-4467-b38d-be6cece520c6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from operator import add\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":3,"id":"38a897f2","metadata":{"id":"38a897f2","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  9 11:39 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":4,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":5,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'project_bucket_sy' \n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)"]},{"cell_type":"code","execution_count":6,"id":"57c101a8","metadata":{"id":"57c101a8","outputId":"b539d532-dc69-4a52-f78b-c4569fb29852","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":7,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import *\n","\n","InvertedIndex.DIR_NAME = \"title_index/postings_gcp_title_index\""]},{"cell_type":"code","execution_count":8,"id":"1pOHyrBdpU-u","metadata":{"id":"1pOHyrBdpU-u"},"outputs":[],"source":["full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name, prefix = 'wiki_files/')\n","for b in blobs:\n","    if b.name != 'wiki_files/graphframes.sh' and b.name != 'wiki_files/' :\n","        paths.append(full_path+b.name)\n"]},{"cell_type":"code","execution_count":9,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"177a3310-cb9b-4ec7-c129-b972476aa7c3","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n","\n","inverted_title = InvertedIndex()\n","inverted_title.DIR_NAME = InvertedIndex.DIR_NAME"]},{"cell_type":"code","execution_count":10,"id":"82881fbf","metadata":{"id":"82881fbf","outputId":"3b837a7e-af95-4a42-9467-70cc0d5d1fdf"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Count number of wiki pages\n","N_docs = parquetFile.count()\n","\n","inverted_title.N = N_docs"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"701811af"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":11,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","DIR_NAME = \"title_index/postings_gcp_title_index\"\n","\n","NUM_BUCKETS = 124\n","\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in \n","    `all_stopwords` and return entries that will go into our posting lists. \n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","    A list of (token, (doc_id, tf)) pairs \n","    for example: [(\"Anarchism\", (12, 5)), ...]'''\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    token_counter = Counter(x for x in tokens if x not in all_stopwords)\n","    res = [(token, (id, tf)) for token, tf in token_counter.items()]\n","    return res\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.'''\n","\n","    return list(sorted(unsorted_pl))\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.'''\n","\n","    return postings.map(lambda x: (x[0],len(x[1])))\n","\n","def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out \n","    all posting lists in a bucket to disk, and returns the posting locations for \n","    each bucket. Partitioning should be done through the use of `token2bucket` \n","    above. Writing to disk should use the function  `write_a_posting_list`, a \n","    static method implemented in inverted_index_colab.py under the InvertedIndex \n","    class. \n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.'''\n","\n","    buckets_split = postings.map(lambda x: (token2bucket_id(x[0]),x))\n","    buckets_group = buckets_split.groupByKey()\n","    return buckets_group.map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name))\n","\n","\n","###### add starts here\n","\n","def count_doc_len(doc_id, text):\n","    '''this functen calculates the document len given the dic id and text '''\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    num_tokens = len([x for x in tokens if x not in all_stopwords])\n","    return (doc_id,num_tokens)\n","\n"]},{"cell_type":"code","execution_count":12,"id":"ML-bllXlwrJw","metadata":{"id":"ML-bllXlwrJw","outputId":"e40052d7-437d-49af-c20b-c24abe045510"},"outputs":[],"source":["# now lets count the frequency of each word in title\n","word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))"]},{"cell_type":"code","execution_count":13,"id":"UHqV82Es9Okv","metadata":{"id":"UHqV82Es9Okv"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# calculate the length of each title and saves values in DL where {key = doc_id: value = doc_length}\n","len_docs_title = doc_title_pairs.map(lambda x: count_doc_len(x[1], x[0]))\n","len_docs_title = len_docs_title.collectAsMap()\n","inverted_title.DL = len_docs_title"]},{"cell_type":"code","execution_count":14,"id":"rd70IYopznWz","metadata":{"id":"rd70IYopznWz"},"outputs":[],"source":["#create postings and sort by doc_id\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)"]},{"cell_type":"code","execution_count":15,"id":"KnLdvv0l1eXR","metadata":{"id":"KnLdvv0l1eXR","outputId":"d3ba0cac-b02c-487d-f7a8-1c82fcdf52f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# stores total frequency per term as a counter in \"term_total\" feild\n","total_terms_title = postings_title.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add)\n","inverted_title.term_total = total_terms_title.collectAsMap()"]},{"cell_type":"code","execution_count":16,"id":"EdbZYiEy8Clm","metadata":{"id":"EdbZYiEy8Clm","outputId":"87cdf5ec-895c-4ece-a3e0-f0e401c9447d"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# filtering postings and calculate df for each term. stores it in title_index.df attribute\n","w2df_title = calculate_df(postings_title)\n","w2df_title_dict = w2df_title.collectAsMap()\n","inverted_title.df = w2df_title_dict"]},{"cell_type":"code","execution_count":17,"id":"jxFmamnvAoYA","metadata":{"id":"jxFmamnvAoYA","outputId":"0a312ddd-8dd4-4d04-b574-ab485c1bf1e0"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# partition posting lists and write out\n","_ = partition_postings_and_write(postings_title).collect()"]},{"cell_type":"code","execution_count":18,"id":"ab3296f4","metadata":{"id":"ab3296f4","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_title = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=DIR_NAME):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs_title[k].extend(v)\n","\n","#stores it in title_index.posting_locs attribute\n","inverted_title.posting_locs = super_posting_locs_title"]},{"cell_type":"code","execution_count":19,"id":"a5d2cfb6","metadata":{"id":"a5d2cfb6","outputId":"42363a03-c1e4-4fad-e7ca-f9d0e290ef9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","\\ [1 files][132.9 MiB/132.9 MiB]                                                \n","Operation completed over 1 objects/132.9 MiB.                                    \n"]}],"source":["# write the global stats out\n","inverted_title.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_index/postings_title_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"86d04da3","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}